{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Ticket Classification - Data Exploration\n",
    "\n",
    "This notebook explores the support ticket dataset to understand the data distribution, patterns, and characteristics before building the classification models.\n",
    "\n",
    "## Objectives\n",
    "- Understand the dataset structure and quality\n",
    "- Analyze ticket categories and priority distributions\n",
    "- Explore text patterns and common themes\n",
    "- Identify potential challenges for model training\n",
    "\n",
    "Author: ML Engineering Team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import re\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for better visualizations\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('../data/tickets.csv')\n",
    "\n",
    "print(f\"Dataset Shape: {df.shape}\")\n",
    "print(f\"\\nDataset Info:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display first few rows\n",
    "print(\"First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percentage = (missing_values / len(df)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Count': missing_values,\n",
    "    'Percentage': missing_percentage\n",
    "})\n",
    "print(missing_df[missing_df['Count'] > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Target Variables Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Ticket Type distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "ticket_type_counts = df['Ticket Type'].value_counts()\n",
    "plt.pie(ticket_type_counts.values, labels=ticket_type_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Ticket Type Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "ticket_type_counts.plot(kind='bar')\n",
    "plt.title('Ticket Type Count')\n",
    "plt.xlabel('Ticket Type')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Ticket Type Statistics:\")\n",
    "print(f\"Total categories: {len(ticket_type_counts)}\")\n",
    "print(f\"Most common: {ticket_type_counts.index[0]} ({ticket_type_counts.iloc[0]} tickets)\")\n",
    "print(f\"Least common: {ticket_type_counts.index[-1]} ({ticket_type_counts.iloc[-1]} tickets)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze Ticket Priority distribution\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "priority_counts = df['Ticket Priority'].value_counts()\n",
    "plt.pie(priority_counts.values, labels=priority_counts.index, autopct='%1.1f%%')\n",
    "plt.title('Ticket Priority Distribution')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "priority_counts.plot(kind='bar')\n",
    "plt.title('Ticket Priority Count')\n",
    "plt.xlabel('Priority')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Priority Statistics:\")\n",
    "print(f\"Total priority levels: {len(priority_counts)}\")\n",
    "print(f\"Balance ratio: {priority_counts.max() / priority_counts.min():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-analysis: Ticket Type vs Priority\n",
    "plt.figure(figsize=(14, 8))\n",
    "cross_tab = pd.crosstab(df['Ticket Type'], df['Ticket Priority'])\n",
    "sns.heatmap(cross_tab, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Ticket Type vs Priority Heatmap')\n",
    "plt.xlabel('Priority')\n",
    "plt.ylabel('Ticket Type')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Cross-tabulation (Ticket Type vs Priority):\")\n",
    "print(cross_tab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Text Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text statistics\n",
    "df['text_length'] = df['Ticket Description'].str.len()\n",
    "df['word_count'] = df['Ticket Description'].str.split().str.len()\n",
    "\n",
    "print(\"Text Statistics:\")\n",
    "print(f\"Average text length: {df['text_length'].mean():.2f} characters\")\n",
    "print(f\"Median text length: {df['text_length'].median():.2f} characters\")\n",
    "print(f\"Average word count: {df['word_count'].mean():.2f} words\")\n",
    "print(f\"Median word count: {df['word_count'].median():.2f} words\")\n",
    "\n",
    "# Plot text length distribution\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.hist(df['text_length'], bins=50, alpha=0.7)\n",
    "plt.title('Text Length Distribution')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.hist(df['word_count'], bins=50, alpha=0.7, color='orange')\n",
    "plt.title('Word Count Distribution')\n",
    "plt.xlabel('Word Count')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.scatter(df['text_length'], df['word_count'], alpha=0.5)\n",
    "plt.title('Text Length vs Word Count')\n",
    "plt.xlabel('Character Count')\n",
    "plt.ylabel('Word Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text length by ticket type\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(data=df, x='Ticket Type', y='text_length')\n",
    "plt.title('Text Length by Ticket Type')\n",
    "plt.xlabel('Ticket Type')\n",
    "plt.ylabel('Character Count')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(data=df, x='Ticket Priority', y='text_length')\n",
    "plt.title('Text Length by Priority')\n",
    "plt.xlabel('Priority')\n",
    "plt.ylabel('Character Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Word Cloud Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple text cleaning function for word clouds\n",
    "def clean_text_for_wc(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Clean all descriptions\n",
    "df['cleaned_text'] = df['Ticket Description'].apply(clean_text_for_wc)\n",
    "\n",
    "# Generate word cloud for all tickets\n",
    "all_text = ' '.join(df['cleaned_text'])\n",
    "wordcloud_all = WordCloud(width=800, height=400, background_color='white').generate(all_text)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.imshow(wordcloud_all, interpolation='bilinear')\n",
    "plt.title('All Tickets Word Cloud')\n",
    "plt.axis('off')\n",
    "\n",
    "# Word clouds by ticket type\n",
    "ticket_types = df['Ticket Type'].unique()[:5]  # Limit to 5 types for visibility\n",
    "for i, ticket_type in enumerate(ticket_types, 2):\n",
    "    if i > 6:  # Limit subplot positions\n",
    "        break\n",
    "    \n",
    "    type_text = ' '.join(df[df['Ticket Type'] == ticket_type]['cleaned_text'])\n",
    "    wordcloud_type = WordCloud(width=400, height=300, background_color='white').generate(type_text)\n",
    "    \n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.imshow(wordcloud_type, interpolation='bilinear')\n",
    "    plt.title(f'{ticket_type} Word Cloud')\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Common Words and Phrases Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get most common words\n",
    "def get_most_common_words(texts, n=20):\n",
    "    all_words = []\n",
    "    for text in texts:\n",
    "        if pd.notna(text):\n",
    "            words = str(text).lower().split()\n",
    "            all_words.extend([word for word in words if len(word) > 2])  # Filter short words\n",
    "    \n",
    "    word_freq = Counter(all_words)\n",
    "    return word_freq.most_common(n)\n",
    "\n",
    "# Get most common words overall\n",
    "common_words = get_most_common_words(df['cleaned_text'], 20)\n",
    "print(\"Top 20 Most Common Words:\")\n",
    "for i, (word, count) in enumerate(common_words, 1):\n",
    "    print(f\"{i:2d}. {word:<15} ({count} occurrences)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most common words by ticket type\n",
    "plt.figure(figsize=(18, 12))\n",
    "\n",
    "for i, ticket_type in enumerate(df['Ticket Type'].unique(), 1):\n",
    "    if i > 6:  # Limit to 6 subplots\n",
    "        break\n",
    "    \n",
    "    type_texts = df[df['Ticket Type'] == ticket_type]['cleaned_text']\n",
    "    common_words_type = get_most_common_words(type_texts, 10)\n",
    "    \n",
    "    words = [word for word, count in common_words_type]\n",
    "    counts = [count for word, count in common_words_type]\n",
    "    \n",
    "    plt.subplot(2, 3, i)\n",
    "    plt.barh(words, counts)\n",
    "    plt.title(f'Top Words: {ticket_type}')\n",
    "    plt.xlabel('Frequency')\n",
    "    \n",
    "    # Reverse y-axis to show highest frequency at top\n",
    "    plt.gca().invert_yaxis()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate tickets\n",
    "duplicate_descriptions = df['Ticket Description'].duplicated().sum()\n",
    "print(f\"Duplicate ticket descriptions: {duplicate_descriptions}\")\n",
    "\n",
    "# Check for very short or empty descriptions\n",
    "very_short = (df['text_length'] < 10).sum()\n",
    "empty_descriptions = (df['text_length'] == 0).sum()\n",
    "\n",
    "print(f\"Very short descriptions (<10 chars): {very_short}\")\n",
    "print(f\"Empty descriptions: {empty_descriptions}\")\n",
    "\n",
    "# Check for potential data quality issues\n",
    "print(f\"\\nData Quality Summary:\")\n",
    "print(f\"- Total tickets: {len(df)}\")\n",
    "print(f\"- Tickets with missing descriptions: {df['Ticket Description'].isnull().sum()}\")\n",
    "print(f\"- Tickets with missing type: {df['Ticket Type'].isnull().sum()}\")\n",
    "print(f\"- Tickets with missing priority: {df['Ticket Priority'].isnull().sum()}\")\n",
    "print(f\"- Average description length: {df['text_length'].mean():.1f} characters\")\n",
    "print(f\"- Class balance (Type): {df['Ticket Type'].value_counts().max() / df['Ticket Type'].value_counts().min():.2f}\")\n",
    "print(f\"- Class balance (Priority): {df['Ticket Priority'].value_counts().max() / df['Ticket Priority'].value_counts().min():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Insights and Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for model planning\n",
    "print(\"=\"*60)\n",
    "print(\"DATA EXPLORATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nðŸ“Š DATASET OVERVIEW:\")\n",
    "print(f\"  Total samples: {len(df):,}\")\n",
    "print(f\"  Features: {len(df.columns)}\")\n",
    "print(f\"  Target variables: Ticket Type ({len(df['Ticket Type'].unique())} classes), Ticket Priority ({len(df['Ticket Priority'].unique())} classes)\")\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CLASSIFICATION CHALLENGES:\")\n",
    "type_balance = df['Ticket Type'].value_counts().max() / df['Ticket Type'].value_counts().min()\n",
    "priority_balance = df['Ticket Priority'].value_counts().max() / df['Ticket Priority'].value_counts().min()\n",
    "\n",
    "print(f\"  Ticket Type balance ratio: {type_balance:.2f} {'(Well balanced)' if type_balance < 2 else '(Imbalanced)'}\")\n",
    "print(f\"  Priority balance ratio: {priority_balance:.2f} {'(Well balanced)' if priority_balance < 2 else '(Imbalanced)'}\")\n",
    "\n",
    "print(f\"\\nðŸ“ TEXT CHARACTERISTICS:\")\n",
    "print(f\"  Average text length: {df['text_length'].mean():.0f} characters\")\n",
    "print(f\"  Average word count: {df['word_count'].mean():.0f} words\")\n",
    "print(f\"  Text length range: {df['text_length'].min()} - {df['text_length'].max()} characters\")\n",
    "\n",
    "print(f\"\\nðŸ”§ MODELING RECOMMENDATIONS:\")\n",
    "print(f\"  1. Use TF-IDF vectorization with n-grams (1,2) for better context\")\n",
    "print(f\"  2. Consider class weighting for priority prediction due to imbalance\")\n",
    "print(f\"  3. Text preprocessing should include stopword removal and lemmatization\")\n",
    "print(f\"  4. Maximum features around 5000 should capture important patterns\")\n",
    "print(f\"  5. Use stratified sampling to maintain class distribution in train/test splits\")\n",
    "\n",
    "print(f\"\\nâš ï¸  DATA QUALITY CONCERNS:\")\n",
    "if duplicate_descriptions > 0:\n",
    "    print(f\"  - {duplicate_descriptions} duplicate descriptions found\")\n",
    "if very_short > 0:\n",
    "    print(f\"  - {very_short} very short descriptions may need special handling\")\n",
    "if df['Ticket Description'].isnull().sum() > 0:\n",
    "    print(f\"  - {df['Ticket Description'].isnull().sum()} missing descriptions\")\n",
    "\n",
    "print(f\"\\nâœ… DATA STRENGTHS:\")\n",
    "print(f\"  - Large dataset size suitable for deep learning\")\n",
    "print(f\"  - Reasonable class balance for both targets\")\n",
    "print(f\"  - Rich text content with diverse vocabulary\")\n",
    "print(f\"  - Clear business problem with well-defined categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Next Steps\n",
    "\n",
    "Based on this exploration, the next steps for building the classification models are:\n",
    "\n",
    "1. **Data Preprocessing**: Implement comprehensive text cleaning\n",
    "2. **Feature Engineering**: Create TF-IDF features with appropriate parameters\n",
    "3. **Model Selection**: Start with Logistic Regression and compare with other algorithms\n",
    "4. **Evaluation**: Use appropriate metrics considering class balance\n",
    "5. **Validation**: Implement cross-validation and hold-out testing\n",
    "6. **Deployment**: Save models and create prediction pipelines\n",
    "\n",
    "The dataset appears suitable for building high-quality classification models with proper preprocessing and feature engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
